#!/usr/bin/python3

def main():
    grammar_b = Grammar()

    grammar = Grammar()
    grammar.rule("cons", "sym", near("num"))
    grammar.rule("cons", "sym")
    #grammar.rule("cons", "sym", "sym")
    grammar_b.rule("cons", "sym", "cons")
    #grammar.rule("cons", "cons", "cons")

    grammar_b.rule("statement", keyword("return"), "cons")

    text = "return a b c d10"
    tokens = list(tokenize(text))

    grammar = grammar + grammar_b
    results = grammar.parse(tokens)
    print("variations:", len(results))
    print("shortest:",   results.shortest)
    for result in results.just(1): # or results
        print(list(result))
        if result.ambiguity == 1:
            print_table(result.traverse(), "results {}".format(result))
        else:
            print_table(result.explain(), "explanation {}".format(result))

    for result in results.just(2): # or results
        print(list(result))
        print_table(result.explain(), "explanation {}".format(result))

class Grammar:
    def __init__(self, un=None, bi=None):
        self.un = un or set()
        self.bi = bi or set()

    def find_unary_rule(self, var, sym):
        for rule in self.un:
            if rule.var == var and rule.sym == sym:
                return rule

    def find_binary_rule(self, var, lhs, rhs):
        for rule in self.bi:
            if rule.var == var and rule.lhs == lhs and rule.rhs == rhs:
                return rule

    def add_unary_rule(self, var, sym):
        rule = self.find_unary_rule(var, sym)
        if not rule:
            rule = UnaryRule(var, sym)
            self.un.add(rule)
        return rule

    def _implicit_rule(self, var, *sequence):
        n = len(sequence)
        if n == 1:
            rule = self.add_unary_rule(var, *sequence)
        if n == 2:
            rule = self.find_binary_rule(var, *sequence)
            if not rule:
                rule = BinaryRule(var, *sequence)
                self.bi.add(rule)
                if isinstance(rule.lhs, Specifier):
                    self.add_unary_rule(rule.lhs, rule.lhs)
                if isinstance(rule.rhs, Specifier):
                    self.add_unary_rule(rule.rhs, rule.rhs)
        if n > 2:
            lhs, *rhs = sequence
            self._implicit_rule(rhs, *rhs)
            rule = self._implicit_rule(var, lhs, rhs)
        return rule

    def rule(self, var, *sequence):
        rule = self._implicit_rule(var, *sequence)
        rule.row = sequence
        return rule

    def parse(self, tokens):
        tab, apl = cyk(tokens, self)
        ntab, shortest = build_ntab(tab)
        return Table(self, tokens, tab, apl, ntab, shortest)

    def __add__(self, other):
        result = Grammar()
        result.un = self.un | other.un

        implicits = {}
        for rule in self.bi:
            if isinstance(rule.var, tuple):
                implicits[rule.var] = rule
            result.bi.add(rule)
        for rule in other.bi:
            if isinstance(rule.var, tuple):
                rule = implicits.get(rule.var, rule)
            result.bi.add(rule)
        return result

# The CYK algorithm that powers this thing.
# There's plenty of information about this algorithm,
# The grammar is given in Chomsky normal form.
# Produces every possible interpretation that is possible with the grammar.
def cyk(tokens, grammar):
    tab = list() # cyk       table
    apl = list() # structure table
    for cols in range(len(tokens), 0, -1):
        tab.append([{} for _ in range(cols)])
        apl.append([[] for _ in range(cols)])
    def increment(cell, key, count=1):
        cell[key] = cell.get(key, 0) + count
    for i, token in enumerate(tokens):
        cell = tab[0][i]
        increment(cell, token.type)
        for rule in grammar.un:
            if rule.match(token):
                increment(cell, rule.var)
                apl[0][i].append((rule, -1))
    for layer in range(1, len(tab)):
        row = tab[layer]
        for i in range(len(tokens) - layer):
            cell = row[i]
            for k in range(layer):
                lcell = lhs_cell(tab, layer, i, k)
                rcell = rhs_cell(tab, layer, i, k)
                for rule in grammar.bi:
                    if rule.lhs in lcell and rule.rhs in rcell:
                        lc = lcell[rule.lhs]
                        rc = rcell[rule.rhs]
                        increment(cell, rule.var, lc*rc)
                        apl[layer][i].append((rule, k))
    return tab, apl

def build_ntab(tab):
    """
    Calculates a map to produce the most concise match first.
    Reveals the shortest match too.
    """
    n = len(tab)
    nom = n+1
    shortest = [nom] * (n+1)
    ntab     = []
    for cols in range(len(tab), 0, -1):
        ntab.append([nom for _ in range(cols)])
    shortest[n]  = 0
    for i in range(n-1, -1, -1):
        score = nom
        for layer in range(n-i):
            if tab[layer][i]:
                length = shortest[i+layer+1] + 1
                ntab[layer][i] = length
                score = min(score, length)
        shortest[i] = score
    return ntab, score

def count_variations(tab):
    """
    Counts the available permutations of parse forests that cover the whole result.
    """
    n     = len(tab)
    count = [1] * (n+1)
    for i in range(n-1, -1, -1):
        score = 0
        for layer in range(n-i):
            if tab[layer][i]:
                score += count[i+layer+1] * len(tab[layer][i])
        count[i] = score
    return count[0]

class Table:
    def __init__(self, grammar, tokens, tab, apl, ntab, shortest):
        self.grammar = grammar
        self.tokens  = tokens
        self.tab = tab
        self.apl = apl
        self.ntab     = ntab
        self.shortest = shortest
        self._length = None

    def __len__(self):
        if self._length is None:
            self._length = count_variations(self.tab)
        return self._length

    def just(self, length):
        yield from iter_results(self, self.tab, self.ntab, length)

    def __iter__(self):
        for i in range(len(self.tab)):
            yield from iter_results(self, self.tab, self.ntab, i+1)

def iter_results(table, tab, ntab, length=1, index=0, prefix=[], p=1):
    n = len(tab)
    if length == 0 and index == n:
        yield Result(table, p, prefix)
    elif length > 0:
        for layer in range(n-length-index, -1, -1):
            for var, count in tab[layer][index].items():
                yield from iter_results(table, tab, ntab, length - 1, index+layer+1, prefix + [(var,layer,count)], p*count)

class Result:
    def __init__(self, table, ambiguity, trees):
        self.table     = table
        self.ambiguity = ambiguity
        self.trees     = trees

    def traverse(self, visitor=None, *args):
        if self.ambiguity > 1:
            raise TypeError("Ambiguous result does not produce unambiguous traversal")
        return traverse(self.table.tab, self.table.apl, self.table.tokens, self.trees, visitor, args)

    def explain(self):
        return explain(self.table.tab, self.table.apl, self.trees)

    def __iter__(self):
        for var, layer, count in self.trees:
            yield var

def traverse(tab, apl, tokens, trees, visitor, args):
    index = 0
    output = []
    if visitor is None:
        visitor = lambda rule, lst: [rule] + lst
    for var, layer, _ in trees:
        output.append(traverse_item(tab, apl, tokens, var, layer, index, visitor, args))
        index += layer+1
    return output

def traverse_item(tab, apl, tokens, var, layer, index, visitor, args):
    if layer == 0:
        rule = None
        for rule, k in apl[layer][index]:
            if rule.var == var:
                break
            rule = None
        if rule and not isinstance(rule.var, Specifier):
            return visitor(rule, [tokens[index]], *args)
        else:
            return tokens[index]

    for rule, k in apl[layer][index]:
        if rule.var == var:
            break
    l_layer, l_index = lhs_cell_coords(layer, index, k)
    left  = traverse_item(tab, apl, tokens, rule.lhs, l_layer, l_index, visitor, args)

    r_layer, r_index = rhs_cell_coords(layer, index, k)
    right = traverse_item(tab, apl, tokens, rule.rhs, r_layer, r_index, visitor, args)
    if isinstance(rule.rhs, tuple):
        return visitor(rule, [left] + right[1:], *args)
    else:
        return visitor(rule, [left, right], *args)

def explain(tab, apl, trees):
    index  = 0
    output = []
    for var, layer, _ in trees:
        rules = []
        for rule, k in apl[layer][index]:
            if rule.var == var:
                rules.append(Explanation(rule, index, layer+1, k+1))
        index += layer+1
        output.append(rules)
    return output

class Explanation:
    def __init__(self, rule, index, length, middle):
        self.rule   = rule
        self.index  = index
        self.middle = middle 
        self.length = length

    def __repr__(self):
        return "{0.rule}:{0.index}:{0.middle}:{0.length}".format(self)

# layer, k - the layer of the left-side.
# this way the k and the layer number is the only thing needed to traverse the parsing result.
def lhs_cell(tab, layer, i, k):
    return tab[k][i]

def rhs_cell(tab, layer, i, k):
    return tab[layer-1-k][i+k+1]

def lhs_cell_coords(layer, i, k):
    return k, i

def rhs_cell_coords(layer, i, k):
    return layer-1-k, i+k+1

class UnaryRule:
    def __init__(self, var, sym):
        self.var = var
        self.sym = sym
        self.row = None

    def __repr__(self):
        return "{0.var} = {0.sym}".format(self)

    def match(self, token):
        if isinstance(self.sym, Specifier):
            return self.sym.match(token)
        return self.sym == token.type

    def __iter__(self):
        return iter(self.row)

class BinaryRule:
    def __init__(self, var, lhs, rhs):
        self.var = var
        self.lhs = lhs
        self.rhs = rhs
        self.row = None

    def __repr__(self):
        return "{0.var} = {0.lhs} {0.rhs}".format(self)

    def __iter__(self):
        return iter(self.row)

# Specifiers extend the capabilities of the engine.
class Specifier:
    pass

class near(Specifier):
    def __init__(self, sym):
        self.sym = sym

    def match(self, token):
        return token.near and token.type == self.sym

    def __eq__(self, other):
        return type(self) == type(other) and self.sym == other.sym

    def __hash__(self):
        return hash((type(self), self.sym))

    def __repr__(self):
        return "near({})".format(self.sym)

class far(Specifier):
    def __init__(self, sym):
        self.sym = sym

    def match(self, token):
        return (not token.near) and token.type == self.sym

    def __eq__(self, other):
        return type(self) == type(other) and self.sym == other.sym

    def __hash__(self):
        return hash((type(self), self.sym))

    def __repr__(self):
        return "far({})".format(self.sym)

class keyword(Specifier):
    def __init__(self, val, type="sym"):
        self.val  = val
        self.type = type

    def match(self, token):
        return self.val == token.val and self.type == token.type

    def __eq__(self, other):
        return type(self) == type(other) and self.val == other.val and self.type == other.type

    def __hash__(self):
        return hash((type(self), self.val, self.type))

    def __repr__(self):
        return "keyword({}, {})".format(self.val, self.type)

def tokenize(text, location=1000):
    ch  = None
    pos = location - 2
    near = True
    def advance():
        nonlocal ch, text, pos
        last = ch
        ch   = text[:1]
        text = text[1:]
        pos += 1
        if last == '\n':
            pos = 1000 + pos // 1000 * 1000
        return last
    def token(type, val):
        nonlocal near
        w = near
        near  = True
        return Token(pos, type, val, w)
    advance()
    while ch:
        string = ""
        if issym(ch):
            while issym(ch):
                string += advance()
            yield token("sym", string)
        elif ch == " ":
            while ch == " ":
                string += advance()
            near = False
        elif isnum(ch):
            while isnum(ch):
                string += advance()
            yield token("num", int(string))

issym   = lambda text: text.isalpha()
isnum   = lambda text: text.isdigit()
isspace = lambda text: text.isspace()

class Token:
    def __init__(self, pos, type, val, near):
        self.pos  = pos
        self.type = type
        self.val  = val
        self.near = near

    def __repr__(self):
        return "{0.type} {0.val!r} at {0.pos}".format(self)

def print_table(tab, header=None):
    if header:
        print("=== {} ===".format(header))
    for row in tab:
        print(row)
    print()

if __name__=='__main__':
    main()
